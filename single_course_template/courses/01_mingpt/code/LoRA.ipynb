{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430622cf-f25d-4177-b97f-e99c337d23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Requirement already satisfied: regex in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: download in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.3.5)\n",
      "Requirement already satisfied: rouge_score in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.1.2)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from download->-r requirements.txt (line 2)) (4.66.3)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from download->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from download->-r requirements.txt (line 2)) (2.32.2)\n",
      "Requirement already satisfied: absl-py in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: nltk in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 3)) (1.23.0)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 3)) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->download->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->download->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->download->-r requirements.txt (line 2)) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->download->-r requirements.txt (line 2)) (2025.6.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe770727-e613-4890-8a85-e1472662f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2038:281473227190448,MainProcess):2025-10-30-14:31:21.909.010 [mindspore/run_check/_check_version.py:412] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import mindspore as ms \n",
    "import mindspore.nn as nn\n",
    "import mindspore.mint as mint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from download import download\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, print_param_report\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.lora_inject import inject_lora_into_gpt, attach_lora_optimizer, mark_only_lora_as_trainable\n",
    "from metrics.perplexity import evaluate_model\n",
    "from metrics.rouge import eval_rouge\n",
    "from mindspore.common.initializer import initializer, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a554c1d4-3a26-4fa8-b811-3f2b5e194ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/config/nbstart_hccl.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 华为云端跑的话需要运行这段代码\n",
    "import os\n",
    "\n",
    "# 保存原始值（可选，用于后续恢复）\n",
    "original_rank_table = os.environ.get('RANK_TABLE_FILE')\n",
    "\n",
    "# 移除环境变量\n",
    "os.environ.pop('RANK_TABLE_FILE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532d446d-0f3c-4895-9c4f-be260b0d56f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(2038:281473227190448,MainProcess):2025-10-30-14:31:34.830.454 [mindspore/run_check/_check_version.py:412] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "# 在本地跑可以更换为\"CPU\"\n",
    "ms.set_device(\"Ascend\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723c4900-ac36-4ddf-9492-ca7a69187f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BPE tokens: 338,025\n"
     ]
    }
   ],
   "source": [
    "tok = BPETokenizer()\n",
    "\n",
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "ids = tok(raw_text).asnumpy()[0].astype(np.int32)   # shape: (T,)\n",
    "print(f\"Total BPE tokens: {len(ids):,}\")\n",
    "\n",
    "# 训练集/验证集切分\n",
    "val_ratio = 0.1\n",
    "split = int(len(ids) * (1.0 - val_ratio))\n",
    "train_ids = ids[:split]\n",
    "val_ids   = ids[split:]\n",
    "\n",
    "block_size = 1024\n",
    "\n",
    "class BPEDataset:\n",
    "    \"\"\"把一长串token切成很多(x,y)训练样本；自回归：预测下一个token\"\"\"\n",
    "    def __init__(self, arr, block_size):\n",
    "        self.arr = arr\n",
    "        self.block = block_size\n",
    "    def __len__(self):\n",
    "        # x: [i, i+block-1], y: [i+1, i+block]\n",
    "        return max(0, len(self.arr) - self.block - 1)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.arr[idx:idx+self.block]\n",
    "        y = self.arr[idx+1:idx+1+self.block]\n",
    "        return ms.Tensor(x, ms.int32), ms.Tensor(y, ms.int32)\n",
    "\n",
    "train_dataset = BPEDataset(train_ids, block_size)\n",
    "val_dataset = BPEDataset(val_ids, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4627aa-d1ab-4c53-94a4-54024f7f3597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace is False and data exists, so doing nothing. Use replace=True to re-download the data.\n",
      "gpt2 模型权重下载成功 --> gpt2.ckpt\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'gpt2'\n",
    "assert MODEL_TYPE in {'gpt2', 'gpt2-medium', 'gpt2-large'}\n",
    "\n",
    "# 模型权重下载\n",
    "url  = f\"https://modelers.cn/coderepo/web/v1/file/MindSpore-Lab/minGPT/main/media/{MODEL_TYPE}.ckpt\"\n",
    "CKPT_PATH = f\"{MODEL_TYPE}.ckpt\"\n",
    "download(url=url, path=CKPT_PATH, replace=False)\n",
    "print(f\"{MODEL_TYPE} 模型权重下载成功 --> {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1529dca-e249-4e79-84bf-4877c8b7488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(2038,ffff97b870b0,python):2025-10-30-14:31:38.009.215 [mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_vmm_adapter.h:150] CheckVmmDriverVersion] Open file /etc/ascend_install.info failed.\n",
      "[WARNING] DEVICE(2038,ffff97b870b0,python):2025-10-30-14:31:38.009.308 [mindspore/ccsrc/plugin/res_manager/ascend/mem_manager/ascend_vmm_adapter.h:189] CheckVmmDriverVersion] Driver version is less than 24.0.0, vmm is disabled by default, drvier_version: 23.0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['h.0.attn.bias',\n",
       "  'h.1.attn.bias',\n",
       "  'h.2.attn.bias',\n",
       "  'h.3.attn.bias',\n",
       "  'h.4.attn.bias',\n",
       "  'h.5.attn.bias',\n",
       "  'h.6.attn.bias',\n",
       "  'h.7.attn.bias',\n",
       "  'h.8.attn.bias',\n",
       "  'h.9.attn.bias',\n",
       "  'h.10.attn.bias',\n",
       "  'h.11.attn.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型实例化\n",
    "C = GPT.get_default_config()\n",
    "C.model_type  = MODEL_TYPE\n",
    "C.vocab_size  = 50257\n",
    "C.block_size  = 1024\n",
    "C.embd_pdrop = 0.0\n",
    "C.resid_pdrop = 0.0\n",
    "C.attn_pdrop  = 0.0\n",
    "model_lora = GPT(C)\n",
    "\n",
    "# 模型权重加载\n",
    "param_dict = ms.load_checkpoint(CKPT_PATH)\n",
    "ms.load_param_into_net(model_lora, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5850c24b-4de0-4bb0-ad35-0d1e901d1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入LoRA层\n",
    "inject_lora_into_gpt(\n",
    "    model_lora,\n",
    "    r=8,\n",
    "    alpha=8.0,\n",
    "    dropout=0.0,\n",
    "    target=(\"qkv\", \"attn_out\", \"mlp\"),\n",
    "    include_lm_head=False\n",
    ")\n",
    "\n",
    "# 只训 LoRA\n",
    "mark_only_lora_as_trainable(model_lora)\n",
    "attach_lora_optimizer(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c6c4fd-262e-4102-8ee1-27100bae9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练配置（LoRA参数微调）\n",
    "T_lora = Trainer.get_default_config()\n",
    "T_lora.max_iters      = 10\n",
    "T_lora.batch_size     = 8\n",
    "T_lora.learning_rate  = 1e-3\n",
    "T_lora.weight_decay   = 0.0\n",
    "T_lora.num_workers    = 2\n",
    "\n",
    "trainer_lora = Trainer(T_lora, model_lora, train_dataset)\n",
    "\n",
    "iters_lora, losses_lora = [], []\n",
    "\n",
    "# 日志输出（每100步）+保存权重（每500步）\n",
    "os.makedirs(\"out-lora\", exist_ok=True)\n",
    "\n",
    "def cb(tr):\n",
    "    if tr.iter_num % 1 == 0:\n",
    "        batch_size = tr.config.batch_size\n",
    "        # 打印训练损失，训练时长和每次批次的数据量\n",
    "        print(f\"iter:{tr.iter_num:5d} | loss:{tr.loss.asnumpy():.4f} | dt:{tr.iter_dt*1000:.1f}ms | batch size: {batch_size}\")\n",
    "        iters_lora.append(tr.iter_num)\n",
    "        losses_lora.append(float(tr.loss.asnumpy()))\n",
    "\n",
    "    if tr.iter_num % 500 == 0 and tr.iter_num > 0:\n",
    "        path = f\"out-lora/lora_gpt2_shakespeare_step{tr.iter_num}.ckpt\"\n",
    "        ms.save_checkpoint(model_lora, path)\n",
    "        print(f\"Saved step ckpt -> {path}\")\n",
    "        \n",
    "trainer_lora.add_callback('on_batch_end', cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3f0e63-092a-4215-9f36-badbfc53ec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter (name=c_attn.lora_0.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_0.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_1.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_1.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_2.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_2.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_3.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_3.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_4.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_4.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_5.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_5.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_6.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_6.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_7.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_7.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_8.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_8.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_9.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_9.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_10.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_10.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_11.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_11.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_12.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_12.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_13.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_13.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_14.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_14.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_15.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_15.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_16.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_16.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_17.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_17.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_18.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_18.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_19.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_19.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_20.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_20.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_21.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_21.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_22.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_22.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_23.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_23.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_24.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_24.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_25.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_25.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_26.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_26.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_27.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_27.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_28.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_28.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_29.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_29.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_30.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_30.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_31.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_31.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_32.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_32.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_33.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_33.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_34.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_34.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_35.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_35.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_36.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_36.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_37.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_37.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_38.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_38.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_39.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_39.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_40.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_40.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_41.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_41.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_42.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_42.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_43.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_43.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_44.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_attn.lora_44.B, shape=(2304, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_45.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_45.B, shape=(768, 8), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_46.A, shape=(8, 768), dtype=Float32, requires_grad=True), Parameter (name=c_fc.lora_46.B, shape=(3072, 8), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_47.A, shape=(8, 3072), dtype=Float32, requires_grad=True), Parameter (name=c_proj.lora_47.B, shape=(768, 8), dtype=Float32, requires_grad=True))\n",
      "...iter:    0 | loss:4.1524 | dt:0.0ms | batch size: 8\n",
      "iter:    1 | loss:9.1235 | dt:40250.8ms | batch size: 8\n",
      "iter:    2 | loss:7.5706 | dt:765.0ms | batch size: 8\n",
      "iter:    3 | loss:8.3069 | dt:738.1ms | batch size: 8\n",
      "iter:    4 | loss:11.7967 | dt:738.0ms | batch size: 8\n",
      "iter:    5 | loss:8.3333 | dt:738.4ms | batch size: 8\n",
      "iter:    6 | loss:7.3985 | dt:738.7ms | batch size: 8\n",
      "iter:    7 | loss:7.5544 | dt:738.2ms | batch size: 8\n",
      "iter:    8 | loss:7.5931 | dt:736.5ms | batch size: 8\n",
      "iter:    9 | loss:7.1102 | dt:738.0ms | batch size: 8\n",
      "Saved final ckpt -> out-lora/lora_gpt2_shakespeare_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "t0_lora = time.time()\n",
    "# 训练（LoRA微调）\n",
    "trainer_lora.run()\n",
    "\n",
    "final_ckpt = \"out-lora/lora_gpt2_shakespeare_final.ckpt\" \n",
    "ms.save_checkpoint(model_lora, final_ckpt)\n",
    "print(f\"Saved final ckpt -> {final_ckpt}\")\n",
    "\n",
    "t_lora = time.time() - t0_lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
